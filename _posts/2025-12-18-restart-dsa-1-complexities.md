---
layout: post
title: "(Re)Start Learning Data Structure and Algorithm - Part 1, Complexities"
date: 2025-12-18
categories: [dsa]
tags: [software engineer]
author: Batiar
---

Big O notation is used to describe the efficiency of an algorithm by measuring how its execution time or space requirements grow as the input size (ğ‘›) increases.

## Common Time Complexities (Fastest to Slowest)

> This list ranks common algorithm complexities from most efficient to least efficient.

| Complexity      | Name         | Description                                                 | Examples                                                                       |
| --------------- | ------------ | ----------------------------------------------------------- | ------------------------------------------------------------------------------ |
| **ğ‘‚(1)**        | Constant     | Runtime does not change with input size.                    | Accessing an array index, pushing/popping from a stack.                        |
| **ğ‘‚(logğ‘›)**     | Logarithmic  | Runtime grows slowly; input size is halved each step.       | Binary Search.                                                                 |
| **ğ‘‚(\sqrt{ğ‘›})** | Square Root  | Efficiently narrows down search space for math-heavy tasks. | Trial Division (Primality Test), Finding all divisors.                         |
| **ğ‘‚(ğ‘›)**        | Linear       | Runtime grows proportionally to input size.                 | Simple loop, linear search.                                                    |
| **ğ‘‚(ğ‘›logğ‘›)**    | Linearithmic | Slightly more than linear; common in efficient sorting.     | Merge Sort, Heap Sort.                                                         |
| **ğ‘‚(n \* m)**   | Bi-Linear    | Time depends on two different input sizes.                  | Comparing every item in List A to List B, Matrix Multiplication (Rectangular). |
| **ğ‘‚(ğ‘›^2)**      | Quadratic    | Runtime grows with the square of the input.                 | Nested loops, Bubble Sort, Insertion Sort.                                     |
| **ğ‘‚(ğ‘›^3)**      | Cubic        | Three nested loops. Becomes very slow quickly.              | Naive Matrix Multiplication, Triple nested loops.                              |
| **ğ‘‚(c^ğ‘›)**      | Exponential  | Time grows by a constant base (2^n, 3^n...).                | Recursive Fibonacci, power set generation.                                     |
| **ğ‘‚(ğ‘›!)**       | Factorial    | Growth is massive even for small ğ‘›.                         | Permutations of a string, Traveling Salesperson problem.                       |

## Data Structure Operations (Average Case)

Common operations vary in efficiency based on the underlying structure.

| Data Structure         | Access  | Search  | Insertion | Deletion |
| ---------------------- | ------- | ------- | --------- | -------- |
| **Array**              | ğ‘‚(1)    | ğ‘‚(ğ‘›)    | ğ‘‚(ğ‘›)      | ğ‘‚(ğ‘›)     |
| **Stack**              | ğ‘‚(ğ‘›)    | ğ‘‚(ğ‘›)    | ğ‘‚(1)      | ğ‘‚(1)     |
| **Queue**              | ğ‘‚(ğ‘›)    | ğ‘‚(ğ‘›)    | ğ‘‚(1)      | ğ‘‚(1)     |
| **Linked List**        | ğ‘‚(ğ‘›)    | ğ‘‚(ğ‘›)    | ğ‘‚(1)      | ğ‘‚(1)     |
| **Hash Table**         | N/A     | ğ‘‚(1)    | ğ‘‚(1)      | ğ‘‚(1)     |
| **Binary Search Tree** | ğ‘‚(logğ‘›) | ğ‘‚(logğ‘›) | ğ‘‚(logğ‘›)   | ğ‘‚(logğ‘›)  |

## Sorting Algorithm Complexity

Efficiency often depends on whether you are looking at the best, average, or worst-case scenario.

| Algorithm          | Time (Best) | Time (Average) | Time (Worst) | Space   |
| ------------------ | ----------- | -------------- | ------------ | ------- |
| **Quicksort**      | ğ‘‚(ğ‘›logğ‘›)    | ğ‘‚(ğ‘›logğ‘›)       | ğ‘‚(ğ‘›^2)       | ğ‘‚(logğ‘›) |
| **Mergesort**      | ğ‘‚(ğ‘›logğ‘›)    | ğ‘‚(ğ‘›logğ‘›)       | ğ‘‚(ğ‘›logğ‘›)     | ğ‘‚(ğ‘›)    |
| **Heapsort**       | ğ‘‚(ğ‘›logğ‘›)    | ğ‘‚(ğ‘›logğ‘›)       | ğ‘‚(ğ‘›logğ‘›)     | ğ‘‚(1)    |
| **Bubble Sort**    | ğ‘‚(ğ‘›)        | ğ‘‚(ğ‘›^2)         | ğ‘‚(ğ‘›^2)       | ğ‘‚(1)    |
| **Insertion Sort** | ğ‘‚(ğ‘›)        | ğ‘‚(ğ‘›^2)         | ğ‘‚(ğ‘›^2)       | ğ‘‚(1)    |

## Big O Rules of Thumb

- **Worst Case**: Big O specifically measures the **upper bound** or worst-case scenario.
- **Iterative Loops**: A single loop is ğ‘‚(ğ‘›). Nested loops are ğ‘‚(ğ‘›^ğ‘˜), where ğ‘˜ is the number of levels.

2. **Divide and Conquer**: Whenever you divide the problem in half every step, it involves logğ‘›.
3. **Recursive Branching**: If a recursive function calls itself twice, it's often ğ‘‚(2^ğ‘›).
4. **Bitwise Operations**: Most bitwise operations (AND, OR, XOR) are ğ‘‚(1).
5. **Ignore the "small" stuff**: ğ‘‚(ğ‘›3+ğ‘›2+ğ‘›) simplifies to ğ‘‚(ğ‘›3) because the ğ‘›^3 term grows so much faster that the others become irrelevant at scale.

For more detail, you can check <https://www.bigocheatsheet.com/>.

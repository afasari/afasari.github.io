---
layout: post
title: "(Re)Start Learning Data Structure and Algorithm - Part 1, Complexities"
date: 2025-12-18
categories: [dsa]
tags: [software engineer]
author: Batiar
---

Big O notation is used to describe the efficiency of an algorithm by measuring how its execution time or space requirements grow as the input size (ğ‘›) increases.

## Common Time Complexities (Fastest to Slowest)

> This list ranks common algorithm complexities from most efficient to least efficient.

| Complexity   | Name         | Description                                             | Examples                                                 |
| ------------ | ------------ | ------------------------------------------------------- | -------------------------------------------------------- |
| **ğ‘‚(1)**     | Constant     | Runtime does not change with input size.                | Accessing an array index, pushing/popping from a stack.  |
| **ğ‘‚(logğ‘›)**  | Logarithmic  | Runtime grows slowly; input size is halved each step.   | Binary Search.                                           |
| **ğ‘‚(ğ‘›)**     | Linear       | Runtime grows proportionally to input size.             | Simple loop, linear search.                              |
| **ğ‘‚(ğ‘›logğ‘›)** | Linearithmic | Slightly more than linear; common in efficient sorting. | Merge Sort, Heap Sort.                                   |
| **ğ‘‚(ğ‘›^2)**   | Quadratic    | Runtime grows with the square of the input.             | Nested loops, Bubble Sort, Insertion Sort.               |
| **ğ‘‚(2ğ‘›)**    | Exponential  | Runtime doubles with each new input element.            | Recursive Fibonacci, power set generation.               |
| **ğ‘‚(ğ‘›!)**    | Factorial    | Growth is massive even for small ğ‘›.                     | Permutations of a string, Traveling Salesperson problem. |

## Data Structure Operations (Average Case)

Common operations vary in efficiency based on the underlying structure.

| Data Structure         | Access  | Search  | Insertion | Deletion |
| ---------------------- | ------- | ------- | --------- | -------- |
| **Array**              | ğ‘‚(1)    | ğ‘‚(ğ‘›)    | ğ‘‚(ğ‘›)      | ğ‘‚(ğ‘›)     |
| **Stack**              | ğ‘‚(ğ‘›)    | ğ‘‚(ğ‘›)    | ğ‘‚(1)      | ğ‘‚(1)     |
| **Queue**              | ğ‘‚(ğ‘›)    | ğ‘‚(ğ‘›)    | ğ‘‚(1)      | ğ‘‚(1)     |
| **Linked List**        | ğ‘‚(ğ‘›)    | ğ‘‚(ğ‘›)    | ğ‘‚(1)      | ğ‘‚(1)     |
| **Hash Table**         | N/A     | ğ‘‚(1)    | ğ‘‚(1)      | ğ‘‚(1)     |
| **Binary Search Tree** | ğ‘‚(logğ‘›) | ğ‘‚(logğ‘›) | ğ‘‚(logğ‘›)   | ğ‘‚(logğ‘›)  |

## Sorting Algorithm Complexity

Efficiency often depends on whether you are looking at the best, average, or worst-case scenario.

| Algorithm          | Time (Best) | Time (Average) | Time (Worst) | Space   |
| ------------------ | ----------- | -------------- | ------------ | ------- |
| **Quicksort**      | ğ‘‚(ğ‘›logğ‘›)    | ğ‘‚(ğ‘›logğ‘›)       | ğ‘‚(ğ‘›^2)       | ğ‘‚(logğ‘›) |
| **Mergesort**      | ğ‘‚(ğ‘›logğ‘›)    | ğ‘‚(ğ‘›logğ‘›)       | ğ‘‚(ğ‘›logğ‘›)     | ğ‘‚(ğ‘›)    |
| **Heapsort**       | ğ‘‚(ğ‘›logğ‘›)    | ğ‘‚(ğ‘›logğ‘›)       | ğ‘‚(ğ‘›logğ‘›)     | ğ‘‚(1)    |
| **Bubble Sort**    | ğ‘‚(ğ‘›)        | ğ‘‚(ğ‘›^2)         | ğ‘‚(ğ‘›^2)       | ğ‘‚(1)    |
| **Insertion Sort** | ğ‘‚(ğ‘›)        | ğ‘‚(ğ‘›^2)         | ğ‘‚(ğ‘›^2)       | ğ‘‚(1)    |

## Big O Rules of Thumb

- **Drop Constants**: ğ‘‚(2ğ‘›) becomes ğ‘‚(ğ‘›).
- **Drop Non-Dominant Terms**: ğ‘‚(ğ‘›^2+ğ‘›) becomes ğ‘‚(ğ‘›^2).
- **Worst Case**: Big O specifically measures the **upper bound** or worst-case scenario.

For more detail, you can check <https://www.bigocheatsheet.com/>.
